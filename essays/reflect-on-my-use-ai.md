---
layout: essay
type: essay
title: "Reflection on My Use of AI in ICS 314"
# All dates must be YYYY-MM-DD format!
date: 2025-5-5
published: true
labels:
  - Software Engineering
  - Learning
---

<img width="500px" class="rounded float-start pe-4" src="../img/eslint.jpeg">

<strong>I. Introduction</strong>
Artificial Intelligence (AI) has become a powerful tool in education, especially in software engineering, where problem-solving, coding, and conceptual understanding are core. In ICS 314, I explored multiple AI tools such as ChatGPT, GitHub Copilot, and Google Bard. These tools aided me in understanding course content, generating code, debugging, and even learning abstract concepts. My interaction with AI spanned across many areas of the course, offering both support and challenge as I navigated the material.

<strong>II. Personal Experience with AI</strong>

Experience WODs (e.g. E18): I used ChatGPT to reinforce functional programming knowledge. For the Functional Programming WOD, I prompted: “Write a function using underscore to implement the following instructions .” The result helped me identify the correct use of .map() and .reduce() but required edits to suit the specific requirements.

In-class Practice WODs: I did not use AI here because I wanted to simulate a real test environment. Practicing independently helped me solidify concepts without relying on external input.

In-class WODs: Similar to the Practice WODs, I avoided AI to challenge myself to perform under time constraints. This built my confidence and speed in solving problems manually.

Essays: I asked ChatGPT to help brainstorm outlines and critique drafts. For instance, I used the prompt: “Help me structure an essay on why coding standards are critical in team software projects.” It provided solid scaffolding, although I rewrote the entire essay in my own words.

Final Project: AI tools were essential here. GitHub Copilot helped autogenerate code snippets, especially boilerplate Meteor code and schema validations. ChatGPT was handy for fixing bugs. When stuck, I asked, “Why is my Uniforms form not submitting in Meteor?” It guided me through troubleshooting steps, though I still needed to cross-reference documentation.

Learning a concept / tutorial: I used Bard to clarify the difference between reactive data sources and useTracker in Meteor. Bard's ability to summarize documentation helped me absorb information more efficiently than skimming through long guides.

Answering a question in class or in Discord: After class, I used ChatGPT to revisit unanswered class questions. For example, someone asked about race conditions in Meteor, and I prompted ChatGPT: “Explain race conditions in Meteor in simple terms.” It helped me understand the issue well enough to follow up in Discord.

Asking or answering a smart-question: Before posting in Discord, I validated my thoughts with ChatGPT. One case: I wanted to ask why my React component didn’t render after navigating routes. I asked ChatGPT: “Why would Meteor with React not rerender on route change?” The answer gave me confidence to post a refined question.

Coding example (e.g., Underscore .pluck):  prompted ChatGPT: “Give a coding example using _.pluck on an array of users.” It instantly returned working code, which I then modified. This helped me quickly understand method functionality.

Explaining code: When encountering complex code like nested Meteor publications, I used the prompt: “Explain this Meteor.subscribe and publish function block.” ChatGPT simplified the logic, helping me understand execution flow.

Writing code: Copilot significantly helped with repetitive tasks, like writing route logic or generating form templates. However, it sometimes made incorrect assumptions or produced obsolete Meteor syntax, which I had to correct.

Documenting code: I used ChatGPT to help generate JSDoc comments. For instance, I prompted: “Write JSDoc for a function that filters users by roles in Meteor.” It gave well-structured comments that I reviewed and edited.

Quality assurance (e.g., ESLint errors): I copied error messages into ChatGPT with prompts like, “Fix this ESLint error in Meteor React.” It provided helpful solutions most of the time, although some suggestions conflicted with the course’s ESLint config.

Other uses in ICS 314 not listed: I used ChatGPT to create dummy test data for forms, generate regex validations, and even simulate user input workflows for edge case testing.

<strong>III. Impact on Learning and Understanding</strong>
AI tools significantly accelerated my learning. They broke down complex topics into digestible parts, offered quick answers, and gave me a springboard for deeper research. However, I noticed that over-relying on them could hinder deep understanding. The key was balancing AI suggestions with my own comprehension and validation.

<strong>IV. Practical Applications</strong>
Outside ICS 314, I used ChatGPT to help prototype a Firebase-based student portal app for a side project. It sped up development by suggesting structure and debugging server-side functions. In HACC, I used Copilot during a hackathon for rapid iteration. These experiences showed me how AI could speed up real-world development while requiring careful review.

<strong>V. Challenges and Opportunities</strong>
The primary challenge with AI was trusting the output. Especially in Meteor, outdated practices sometimes surfaced. Still, AI offered opportunities to learn by example, gain quick feedback, and brainstorm solutions. The more specific my prompts, the better the results.

<strong>VI. Comparative Analysis</strong>
Traditional methods like lectures and textbooks offer deeper structure and verified accuracy, while AI provides speed and flexibility. With AI, I could explore tangents and receive instant clarification, which made learning more engaging. However, I still relied on traditional learning to ensure foundational understanding.

<strong>VII. Future Considerations</strong>
AI will become increasingly important in software education. I foresee AI being integrated into coding environments for personalized debugging, feedback, and testing. However, students need to learn how to critically evaluate AI outputs. Future coursework should include AI literacy and ethics.

<strong>VIII. Conclusion</strong>
AI greatly influenced my journey through ICS 314. It served as a tutor, debugger, writing assistant, and coding partner. While not a substitute for understanding, it was an invaluable resource. For future integration, I suggest combining AI tools with peer collaboration and instructor guidance to strike the right balance between efficiency and comprehension.
