---
layout: essay
type: essay
title: "Reflection on My Use of AI in ICS 314"
# All dates must be YYYY-MM-DD format!
date: 2025-5-5
published: true
labels:
  - Software Engineering
  - Learning
---

<img width="500px" class="rounded float-start pe-4" src="../img/eslint.jpeg">

<strong>I. Introduction</strong>
Artificial Intelligence (AI) has rapidly become a transformative force in education, especially in technical fields like Software Engineering. Its ability to assist, automate, and analyze has opened new doors for students to enhance their learning. In ICS 314, AI tools such as ChatGPT, GitHub Copilot, and Google Bard played a significant role in how I approached problem-solving, coding, and understanding complex software engineering concepts. These tools were instrumental in both developing and debugging code, generating examples, and guiding my documentation and learning processes.

<strong>II. Personal Experience with AI</strong>

Experience WODs (e.g. E18): For E18, I used ChatGPT to get a clearer understanding of functional programming concepts. My prompt was: "Explain how to use Underscore.js map and reduce in JavaScript." ChatGPT gave concise and usable examples that helped reinforce my understanding before attempting the WOD.

In-class Practice WODs: I avoided AI here to simulate real exam-like environments. I found it important to practice independently without reliance on external help.

In-class WODs: Similar to Practice WODs, I chose not to use AI during timed WODs to ensure that I developed the muscle memory for solving problems quickly on my own.

Essays: For my essays, I asked ChatGPT to help brainstorm outline structures and used it to refine my grammar. For example, I asked, "Give me a clear outline for discussing the role of coding standards in group projects." While the ideas were helpful, I rewrote most parts in my own voice.

Final Project: Copilot was helpful in generating boilerplate code for the Meteor app, particularly for MongoDB schemas and form validation logic. ChatGPT was helpful for syntax-specific questions, such as “How do I validate a form using Uniforms in Meteor?” It saved time, though occasionally gave deprecated or incompatible answers.

Learning a concept / tutorial: When learning about React-Meteor integration, I asked Bard to explain the difference between reactive data sources and useTracker. This helped clarify the conceptual difference more quickly than reading documentation alone.

Answering a question in class or in Discord: I did not use AI during class discussions but occasionally asked ChatGPT after class to clarify someone else's question that I didn’t fully understand at the time.

Asking or answering a smart-question: Before asking a smart-question in Discord, I often used ChatGPT to verify if the question made sense. One example was, “How do I debug an issue where Meteor isn’t rendering my React component after a route change?”

Coding example (e.g., Underscore .pluck): I used ChatGPT to generate small examples, such as "Give an example using _.pluck to extract names from a user array." This was a helpful way to test methods quickly.

Explaining code: I used ChatGPT to help explain some third-party package code to myself. My prompt was: “Explain what this Meteor.subscribe call is doing in simple terms.” It was helpful for breaking down unfamiliar syntax.

Writing code: Copilot was most helpful here. It often auto-completed repetitive code, such as defining new routes or templates. However, it occasionally introduced subtle bugs.

Documenting code: I used ChatGPT to generate JSDoc comments for functions. My prompt: “Write a JSDoc comment for a function that filters users by role.” It saved time, but I reviewed every output for accuracy.

Quality assurance (e.g., ESLint errors): When I encountered ESLint issues I didn’t understand, I copied the error message into ChatGPT and asked, “How do I fix this ESLint error in a Meteor React project?” The response usually pointed me in the right direction.

Other uses in ICS 314 not listed: I also used ChatGPT to create test cases for form inputs and simulate user flows for the final project.

<strong>III. Impact on Learning and Understanding</strong>
Using AI enhanced my learning by speeding up access to relevant explanations, offering examples, and demystifying complex concepts. However, it also posed a risk of surface-level understanding if I relied on it too much. I learned to treat AI as a guide, not a crutch, and always validated the responses.

<strong>IV. Practical Applications</strong>
Outside ICS 314, I used AI to prototype small web applications and scripts for my part-time job. For example, ChatGPT helped generate a quick Node.js server to test API endpoints. This showed me how AI can accelerate real-world development.

<strong>V. Challenges and Opportunities</strong>
A key challenge was verifying the correctness of AI output. Sometimes ChatGPT gave outdated or incorrect information, particularly for Meteor-specific questions. But it also presented opportunities—like using Copilot to learn by observing what it suggests.

<strong>VI. Comparative Analysis</strong>
Compared to traditional learning (lectures, textbooks), AI-enhanced learning was more interactive and on-demand. It allowed me to explore tangents and clarify doubts immediately. However, traditional methods ensured deeper, more structured understanding.

<strong>VII. Future Considerations</strong>
AI will likely become a core component of software engineering education. Its ability to provide instant feedback and adapt to student needs is invaluable. However, educators should also teach students how to critically assess AI responses.

<strong>VIII. Conclusion</strong>
AI was a valuable companion throughout ICS 314. From debugging to documentation, it made my learning process faster and more dynamic. Moving forward, I recommend integrating AI literacy into coursework—teaching students not just how to use AI, but how to use it well.
